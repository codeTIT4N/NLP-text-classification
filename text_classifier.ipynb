{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-gardening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tit4n/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing important libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finished-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "reviews = load_files('txt_sentoken/')\n",
    "#note it will loop through all the different directories contained in this txt_sentoken folder and generate different\n",
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fitted-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to seperate the class and the documents i.e., we need documents in a seperate list and we also need\n",
    "#corresponding classes to be in another list\n",
    "X,y = reviews.data,reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intimate-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n",
    "#here 0 is for neg class because it was first folder in txt_senttoken so data collection is done\n",
    "#note reviews are in X and different classes are in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cloudy-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note if you use load_files for a very large dataset it will take more time for that what we can do is we can store\n",
    "#this X and y as pickle file in python\n",
    "#storing as pickel files\n",
    "with open('X.pickle','wb') as f: #note these are byte type files that is why wb\n",
    "    pickle.dump(X,f)\n",
    "#note this will generate a X.pickle file in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eight-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Y.pickle','wb') as f:\n",
    "    pickle.dump(y,f)\n",
    "#note this is how you persist the different py objects as pickle file and we are going to do the same thing when we\n",
    "#are going to build our classifier because we want to persist our classifier as well for using that at a later point\n",
    "#of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "authentic-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will see how to retrieve object stores in the pickle file\n",
    "#unpickling a data set\n",
    "with open('X.pickle','rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    \n",
    "with open('Y.pickle','rb') as f:\n",
    "    y = pickle.load(f)\n",
    "#right now nothing is going to happen because we alreaded have these files in the memory but we can pretty much load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "heard-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data from dataset\n",
    "corpus = [] #this will be a list of documents\n",
    "for i in range (0,len(X)):\n",
    "    review = re.sub(r'\\W',' ',str(X[i])) #substituting all non word characters with ' '\n",
    "    review = review.lower() #lowering everything\n",
    "    review = re.sub(r'\\s+[a-z]\\s+',' ',review) #removing all single characters beacuse they are not important\n",
    "    review = re.sub(r'^[a-z]\\s+',' ',review) #if a sentence starts with single character it will remove it\n",
    "    review = re.sub(r'\\s+',' ',review) #this is to remove any extra spaces we generated\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "colored-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "#we are going to create the BOW model and then convert it into TFIDF model\n",
    "from sklearn.feature_extraction.text import CountVectorizer #to create a simple BOW model\n",
    "#creating the object of CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features = 2000, min_df=3, max_df=0.6, stop_words = stopwords.words('english'))\n",
    "#when we build the BOW after creating histogram we have to filter it and select the top n features/words and when we \n",
    "#will be building some real world models those models will contain a lot of words so we choose 2000 here it means we\n",
    "#want 2000 most frequent words as features and we will exclude all the rest.\n",
    "#min_df = min document frequency when it will select words from histogram then it will exclude all those words that\n",
    "#appear in 3 documents or less than 3 documents. max_df =max document frequency here we have specified it as percent\n",
    "#0.6 means we are going to exclude all the different words that appear in 60% of the documents or more than that\n",
    "#so we are only focused towards getting the most important words from corpus. stop_words are also specified here we\n",
    "#are telling the CountVectorizer that you will exclude all the words that are contained in this list of stop words.\n",
    "#so after creating the histogram, when it is filtering then it will do these 3 things.\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "collectible-chile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [2 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "#we can see in variable explorer it is a 2000*2000 array\n",
    "#2000 rows is because we have 2000 positive or negative documents in the corpus and 2000 columns mean 2000 diffrent \n",
    "#features. Note: max_features was specified to be 2000 hence 2000 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lonely-struggle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.06887219, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12007883, 0.        , 0.06321361, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will transform this to TFIDF model using prebuilt class\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "important-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to divide this dataset into 2 seperate datasets 1 we will use for traning our model and the other\n",
    "#for testing the model performance\n",
    "#so we will use 1600 for training the model and about 400 for testing using sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train,text_test,sent_train,sent_test = train_test_split(X,y,test_size = 0.2,random_state = 0) \n",
    "#y is list of different classes i.e., 2000 classes with 0's and 1's 0 is negative 1 is positive so for each of the \n",
    "#rows of X we have a y. Here test_size = 0.2 means 80 percent will be for training and 20 percent for testing\n",
    "#its like seed variable for random in c++,jave,etc so anytime we use same seed variable we will get same result\n",
    "#this is going to return 4 different values where text_train is the text or the list of documents that we are going\n",
    "#to be using for training the whole document, text_test is for testing, sent_train are the different sentiment classes\n",
    "#associated with the text_train and sent_test are the different sentiment classes associated with text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blond-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1600x2000'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(len(text_train)) + \"x\" +str(len(text_train[0])) #this is done to show dimentions of the matrix\n",
    "#so here we can see it contains about 1600 different rows and 2000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mechanical-point",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'400x2000'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(len(text_test)) + \"x\" +str(len(text_test[0])) #this is done to show dimentions of the matrix\n",
    "#so here we can see it contains about 400 different rows and 2000 features\n",
    "#so no. of features are constant but we are varying the no. of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "upset-series",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we are going to use the Logistic Regression ML Algorithm for fitting this whole model and building the classifier\n",
    "#now to use logistic regression we need to import logistic regression class from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression() #making object \n",
    "#now we will train our model\n",
    "classifier.fit(text_train,sent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "martial-malawi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we are going to test our model using test set\n",
    "#to store all the predictions\n",
    "sent_pred = classifier.predict(text_test)\n",
    "sent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "casual-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[168,  40],\n",
       "       [ 21, 171]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now note that the result we got is the prediction and we also have actual values in sent_test now we want to compare\n",
    "#the two to find how much accurate our model is compare to humans\n",
    "# to do that we have a very good function in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sent_test,sent_pred)\n",
    "cm\n",
    "#Here through the columns we have the actual values and for the rows we have predicted values\n",
    "#Visualizing it:\n",
    "#   |  0  |  1\n",
    "#  0| 166 | 42\n",
    "#  1| 27  | 165\n",
    "#so what it means is for 166 predictions were really negative and our model has also predicted them correctly\n",
    "#and 27 were actually negative but our model has predicted them to be positive. Now similarly for the positive classes\n",
    "#42 results were actually poitive but our model predicted it wrong and 165 were negative which it predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "filled-absence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the accurary rate we just need to add them to do this\n",
    "cm[0][0] + cm[1][1] #adding correctly predicted values\n",
    "#so we have 331 correct predictions out of 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dominant-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.75"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "331/4  #(331/400)*100 simple percentage calculation\n",
    "#this is the accuracy we got. So, our model is 82.75 % accurate. Whis is pretty go more than 80 percent that too with\n",
    "#only 2000 rows of documents. So if we had like 100k or 200k we will get much higher accuracy or if we change\n",
    "#max_features value we will also get much higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "plastic-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to save our model and vectorizer so that we can use it later. So, we will save this model as a\n",
    "#pickle file and we will import this classifier later in another project.\n",
    "\n",
    "#Pickling the classifier\n",
    "with open('classifier.pickle','wb') as f:\n",
    "    pickle.dump(classifier,f) #note we did this earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "boxed-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we also need to dump another thing because later we can't just do classifier.predict(\"aa bb\") because it expects\n",
    "#that we pass it a vectorized input with 2k features because we have trained it like that so to do that we will also\n",
    "#save the vectorizer but here we haven't use a TFIDF vectorize. Here we have just used a count vectorizer and then\n",
    "#transformed it using a TFIDF transformer. So, if we want to use this stratergy we have to save both the transformer\n",
    "#and vectorizer as pickle file. Just to make it simple we will vectorize this whole corpus again using the TFIDF\n",
    "#vectorizer. So we are not going to train the model again we will just tfidf vectorize it.\n",
    "#Note here we are just going to repeate few thing we did earlier.\n",
    "\n",
    "#unpickling the dataset previously stored in pickle files\n",
    "with open('X.pickle','rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    \n",
    "with open('Y.pickle','rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "#Creating the corpous\n",
    "corpus = [] #this will be a list of documents\n",
    "for i in range (0,len(X)):\n",
    "    review = re.sub(r'\\W',' ',str(X[i])) #substituting all non word characters with ' '\n",
    "    review = review.lower() #lowering everything\n",
    "    review = re.sub(r'\\s+[a-z]\\s+',' ',review) #removing all single characters beacuse they are not important\n",
    "    review = re.sub(r'^[a-z]\\s+',' ',review) #if a sentence starts with single character it will remove it\n",
    "    review = re.sub(r'\\s+',' ',review) #this is to remove any extra spaces we generated\n",
    "    corpus.append(review)\n",
    "\n",
    "#Feeding it to tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 2000, min_df=3, max_df=0.6, stop_words = stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "#note here we haven't retrained the model or retested it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "chinese-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will store this new vectorizer\n",
    "#Pickling the vectorizer\n",
    "with open('tfidfmodel.pickle','wb') as f:\n",
    "    pickle.dump(vectorizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "transparent-studio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will see 2 new files in the pwd\n",
    "#classifier.pickle and tfidfmodel.pickle\n",
    "#now we can use these files in our sentiment anlysis at a later point in time.\n",
    "#now we will import our saved classifier and vectorizer and we will predict weather a simple sentence is positive or\n",
    "#negative. \n",
    "\n",
    "#note usually we will do this in a seperate file where we haven't trained our model but this is just an example.\n",
    "#Unpickling classifier\n",
    "with open('classifier.pickle','rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "lesbian-lithuania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.6, max_features=2000, min_df=3,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unpickling vectorizer\n",
    "with open('tfidfmodel.pickle','rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cultural-disco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#now lets try using it. On a simple sentence.\n",
    "sample = [\"you are a good person\"]\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "#note here we are not fitting rather we are just transforming based on already fitted corpous\n",
    "#what this will do is it will make sample seem like a document from corpous\n",
    "print(clf.predict(sample))\n",
    "#1 means positive so it has predicted it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "brave-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#doing some more predictions\n",
    "sample = [\"you are a bad person\"]\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "print(clf.predict(sample))\n",
    "#0 means negative so it has predicted it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "intensive-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sample = [\"i hate you\"]\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "print(clf.predict(sample))\n",
    "#wrong prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "181px",
    "left": "868px",
    "right": "20px",
    "top": "98px",
    "width": "360px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
